@article{Brown2012,
abstract = {Deep shotgun sequencing and analysis of genomes, transcriptomes, amplified single-cell genomes, and metagenomes has enabled investigation of a wide range of organisms and ecosystems. However, sampling variation in short-read data sets and high sequencing error rates of modern sequencers present many new computational challenges in data interpretation. These challenges have led to the development of new classes of mapping tools and {\{}$\backslash$em de novo{\}} assemblers. These algorithms are challenged by the continued improvement in sequencing throughput. We here describe digital normalization, a single-pass computational algorithm that systematizes coverage in shotgun sequencing data sets, thereby decreasing sampling variation, discarding redundant data, and removing the majority of errors. Digital normalization substantially reduces the size of shotgun data sets and decreases the memory and time requirements for {\{}$\backslash$em de novo{\}} sequence assembly, all without significantly impacting content of the generated contigs. We apply digital normalization to the assembly of microbial genomic data, amplified single-cell genomic data, and transcriptomic data. Our implementation is freely available for use and modification.},
archivePrefix = {arXiv},
arxivId = {1203.4802},
author = {Brown, C. Titus and Howe, Adina and Zhang, Qingpeng and Pyrkosz, Alexis B. and Brom, Timothy H.},
eprint = {1203.4802},
file = {:Users/johnsolk/Library/Application Support/Mendeley Desktop/Downloaded/Brown et al. - 2012 - A Reference-Free Algorithm for Computational Normalization of Shotgun Sequencing Data.pdf:pdf},
month = {mar},
title = {{A Reference-Free Algorithm for Computational Normalization of Shotgun Sequencing Data}},
url = {http://arxiv.org/abs/1203.4802},
year = {2012}
}
